---
title: "week_06_notes"
author: "Andrew Nalundasan"
date: "11/01/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Overview Video 

+ Forecasting with AR model
+ MA Model: How do we estimate the MA model?

    + The book doesn't talk about how to estimate the model, we typically leave that work to R to do for us
    + parameters: mu and theta
        + 1 explanatory variable, eps_t
        + eps_t can be represented with information set along with mu and theta
        + Use Maximum Likelihood Error to estimate mu and theta

+ Find a TS dataset and book meeting with Dr. Xie


## Chapter 7 Lecture Videos - Forecasting with AR Models

+ Increase in unemployment numbers for some period, and decrease for other periods
+ Fluctuation economy <- expansion and recession (micro economy) - boom and bust times
+ business cycle <- macro economy (unemployment rate, poverty rate, etc.)
+ Seasonality <- periodical patterns associated with the calendar

+ AR(1) Process

    + dependent variable Y_t
    + only need to estimate one coefficient, phi
        + as phi increases, the TS becomes more "sticky"
        + mean will stay at some level without returning back to mean
        + if phi < 1 <- stationary (small phi)
            + ACF fades away quickly
        + elif phi > 1 <- random walk (big phi)
            + ACF decays slowly
    + MA model is "short memory"
        + After period 1, ACF becomes 0. This is a sharp cutoff
        
+ AR(2) Process

    + Regress Y_t on past two periods
        + this yields 2 coefficients
        + sum of 2 coefficients <- persistency parameter
            + As persistency parameter increases, the process becomes more "sticky"
                + if persistency parameter == 1 <- random walk
            + As the persistency parameter decreases, it returns to the mean more often
            
    + ACF gradually fades away
        + when persistency parameter is small, ACF fades away faster
        + when persistency parameter is large, ACF lingers longer (ACF sticky)
        + when first parameter is negative, ACF oscilates (positive, negative, positive, negative)

+ Summary

    + AR process: represents current values with past values
    + AR process are not all stationary processes
        + if persistency parameter < 1: stationary
        + if persistency parameter == 1: random walk
    + Determing if TS model is a cycle or seasonality

## Chapter 7 R Instructions

```{r}
# load libraries
library(readxl)
library(forecast) # use package forecast
```

### Part I: simulation AR TS with and without seasonality

+ make simulations of many TS with and without seasonality

```{r}

y <- numeric(200)  # generate a vector of 200 zeros

# skip the 1st observation, hence [2:200]
y[2:200] = arima.sim(n=199, list(ar=0.4), innov=rnorm(199, 1, 0.5))  # generate AR(1) (leave the first obs as zero)
plot.ts(y)
```

**Comments**

+ Using AR(1) model

    + list(ar=0.4) <- this means it's an AR(1) model because only 1 coefficient



```{r}
# AR(2) process
y <- numeric(200) #generate a vector of 200 zeros

y[3:200] = arima.sim(n=198, list(ar = c(1, -.5)),innov = rnorm(198, 1, 1)) 

plot.ts(y)

```

**Comments**

+ innov <- error term

    + normal distribution
    + mean = 1
    + stdev = 1
    
+ since AR(2), take y[3:200]

```{r}
# AR(2) model with seasonality
y <- numeric(84) # generate a vector of 84 zeros

y[5:84] = arima.sim(n=80, list(ar=c(0, 0, 0, 0.8)), innov = rnorm(80, 0, 0.5))

plot.ts(y)
```

**Comments**

+ ar=c(0, 0, 0, .8) <- (Q1, Q2, Q3, Q4). hence, seasonality since dealing with annual quarters
+ First 4 elements are 0, then start with the 5th element, hence [5:84]

```{r}
# AR(8) model, using seasonalities
y2 <- numeric(84) # generate a vector of 84 zeros

y2[9:84] = arima.sim(n=76, list(ar = c(0, 0, 0, 0.5, 0, 0, 0, 0.3)), innov = rnorm(76, 0, 0.5)) 

plot.ts(y2)
```

**Comments**

+ list(ar=c...) <- mimics seasonality for 4 quarters for 2 years. 
+ So 8 quarters with correlations on Q4 of both years

```{r}
# Using MA model
y3 <- 2 + arima.sim(n=84, list(ma=c(0, 0, 0, 0.9)), innov = rnorm(84, 0, 0.5)) # generate S-MA(1)

plot.ts(y3)

```

**Comments**

+ Correlated with Q4 of last year
+ Using Moving Average because 'list(ma=c(...))

```{r}

y4 <- 2 + arima.sim(n=84, list(ma=c(0, 0, 0, -1, 0, 0, 0, 0.25)), innov = rnorm(84, 0, 0.5)) # generate S-MA(2)

plot.ts(y4)

```

**Comments**

+ Data of this quarter to be correlated with the same quarter of last year and 2 years ago

    + list(ma=c(........))
    + MA(8) model because 8 coefficients 

### Part II: estimation 

```{r}
# read in data
data <- read_excel("../02_raw_data/Figure7_11_inflation.xls")

# declare CPI_GR as (yearly) time series (1913-2016)
CPI_GR <- ts(data$CPI_Growth, frequency=1, start=c(1913)) 

# plot the data
plot(CPI_GR)
```

**Comments**

+ Looks like first and possibly second lags are extreme oscilations
+ Later mellows out and stays positive
+ Making forecast of last 6 obs
+ Use obs 1-85 as estimation sample


```{r}
# restrict the sample of CPI_GR up to 90th obs (i.e. 2003 year)
CPI_GRr <- ts(data$CPI_Growth[1:85], frequency=1, start=c(1913))  # obs 1-85 is estimation sample

plot(CPI_GRr)
```

**Comments**

+ This looks identical to the previous plot
+ only difference is this one takes a slice from obs 1-85 (estimation sample)
+ There are 91 obs in this dataset


```{r}
# in-sample AR(2) estimation (1913-2003)
model <- arima(CPI_GRr, order=c(2, 0, 0))

# see the estimation output
model 

```

**Comments**

+ AR1 and AR2 coefficients 
+ Are these persistency parameters?
+ AR2 is negative. This means that there should be an oscillation somewhere
+ Data is positively correlated with last period (ar1 = 0.7919)
+ Negatively correlated with 2 periods ago (ar2 = -0.2466)

### Part III: Forecasting

```{r}
# forecasting code

# dynamic forecast for 6 steps ahead (h=6)
fCPI_GRr <- forecast(model, h=6)  # >>> list of 10

# forecast summary
summary(fCPI_GRr) 

# plot the forecast
plot(fCPI_GRr, include=10) 

# add the actual series to the plot (in magenta color)
# need to run the chunk to see this
lines(CPI_GR, col="red") 

```

**Comments**

+ Makes 6 forecasts
+ 1, 2, 3, 4, 5, 6 steps ahead, hence h=6
+ 6 point forecasts
+ Blue line is point h forecasts
+ Dark blue shading == 95% CI
+ Gray shading == 80% CI

### Part IV: estimation of timeseries with seasonality

```{r}
# read in another dataset
data <- read_excel("../02_raw_data/Figure7_19_constructionchanges.xls")

# declare change_CONST as (monthly) time series
change_CONST <- ts(data$ConstructionChanges, frequency=12, start=c(2002, 1))

# plot the data
plot(change_CONST)

```

**Comments**

+ data is monthly, with 109 obs
+ we use the entire dataset as estimation sample to make the 12 step forecast (12 months)
+ Looks like seasonality to me
+ spikes in months 1 and 3
+ low points in month 12


```{r}
# models
# AR(1) & S-AR(1) estimation
model <- arima(change_CONST, order=c(1, 0, 0), seasonal=list(order=c(1, 0, 0), period=12))  # monthly data (period=12)

# see the estimation output
model

# Akaike IC
AIC(model) 

# Bayesian IC
BIC(model) 
```

**Comments**

+ Coefficients: 

    + ar1 <- y_t-1 (== 0.4524)
        + positive coefficient
        + monthly construction growth rate is positively correlated with growth rate of last month
    + sar1 <- t_t-12 (==0.9034)
        + more positively correlated with the same month of last year (**aha! seasonality**)
    
    
+ AR1 and SAR1 coefficients in 'model'
+ What is 'IC'?

    + I...something Coefficient?
    + AIC vs. BIC
    
### Part V: Forecasting of timeseries with seasonality

```{r}

# dynamic forecast for 12 steps ahead
fchange_CONST <- forecast(model, h=12) 

# forecast summary
summary(fchange_CONST) 

# plot the forecast
plot(fchange_CONST, include=24) 
```

**Comments**

+ Left side is the entire dataset (estimation sample)
+ blue lines are the 12 month forecasts with 90% CI and 80% CI

### Summary

+ Use AR process to generate TS
+ How to use AR model and MA to make estimation and forecasts of TS with and without seasonality


## Chapter 6 External Videos - AR Process

### Autoregressive Model

+ AR processes are very well suited to model cyclical fluctuations
+ Cycle <- fluctuations in the data that happen at periodic intervals
+ Expansion <- positive growth in economic output
+ Recession <- negative or slow growth
+ Bubbles <- exuberant prices
+ Busts <- rapid price decline
+ inflation <- rapid price growth
+ Deflation <- severe price decline
+ deterministic cycle <- common in physical sciences

    + right hand side of equation is a function of time
    
+ Stochastic cycle <- prevalent in economics and business data

    + a cycle is stochastic when it is generated by random variables
    + never a function of time

+ ACF informs about the dependence between random variables in different periods of time
+ the main question in a linear regression is to find out the conditional mean of the variable of interest
+ Autoregressive <- regression model in which the dependent variable and the regressors belong to the same stochastic process
+ For each AR(order) process, we ask the same 3 questions:

    1. WHat does a time series of an AR process look like
    2. What do the correpsonding autocorrelation functions look like?
    3. What is the optimal forecast?
    
+ AR(1) Process

    + AC decays smoothly toward 0
    + PAC has ONE (1) relevant spike
    + phi <- persistence parameter
    
+ Autocorrelation functions of an AR(1) process has 3 distinctive features:

    1. Some sampling error. AC and PAC functions are estimated
    2. AC decreases exponentially toward 0, Decay is faster when persistence parameter (phi) is small. Higher phi has larger autocorrelations. Estimated correlations are practically the same as the population autocorrelations
    3. PACF characterzed by only one spike rho_k for k > 1 is practically 0

+ Density Forecast <- conditional probability density function of the process at the future date

+ Forecasting with an AR(1) is limited by the magnitude of the persistence parameter

    + good to forecast short/medium term
    + in long term, the forecast converges to the unconditional mean of the process

## External Resources - AR Process

### AR(1) Process Introduction and Milk Example

+ Autoregressive: it's a regression. Trying to predict something based on other things
+ Auto: Trying to predict something based on past values of that same thing
+ If we can capture a pattern, it's easier to forecast an accurate prediction
+ If a simpler model can do the same job as a complex model, we prefer the simple model
+ PACF:

    + PACF of lag_3 is the direct correlation of quantity demanded 3 periods ago
        + M_t-3 direct effect on M_t
    + Only want to keep the lags that are high in magnitude (significant - outside the dotted bands)
    + Only include lags that are significant in our model. Throw everything else out

### AR(1) Process 

+ Autoregressive because X_t-1 is regressed on itself
+ Order 1 because only X_t-1 coefficient is included in the model. If X_t-3 were included, it would be AR(3)
+ A shock can have persistent effects
+ MA(1) process only has an effect that lasts for 2 time periods
    
    + where the shock occurs and for the next period
    + AR(1) process, the shock lasts infinitely

### AR(1) Process - properties

+ Stationary weakly dependent

    1. Must have a constant mean
        + Expectation of X_t is constant
        + Expectation of the first term of the series == 0

### AR(1) Process - properties

+ a bunch of math proofs

+ modulus of rho is less than 1

### AR(1) Process - properties

+ Conditions are both covariance and weakly dependent

+ a bunch of math proofs

+ finite covariance structure: abs(rho) < 1

+ correlation = cov(X_t, X_t+h) / var(X_t)

    + must == rho**h

### AR(1) vs. MA(1) processes

+ Determine if we have AR(1) or MA(1)

1. Plot the series

    + must have constant mean at 0
        + if not, it can't be either AR(1) or MA(1)
    + variance of process must be constant over time
    
+ AR(1) <- correlation = rho**h

    + for AR(1) process, correlation exponentially declines
    + after finite number of lags, correlation should be almost 0
    
+ MA(1) <- correlation = theta, h=1 or 1, h>1

    + for MA(1) process, correlation drops to almost 0 after the first lag
    + allows for estimation of parameter theta


### Partial vs Total Correlogram

+ Total
    
    + Correlation between X_t regressed on itself
    + AR(1) decreases exponentially
    + MA(1) sharp decrease below significance level after first lag

+ PACF

    + find the correlation of a variable with itself and subtract it from the model
    + then plot the residual 
    + AR(1) has strong first autocorrelation at first lag
        + Following lags will sharply decrease close to 0
    + AR(2) strong correlation at 1st and 2nd lag
        + Following lags will sharply decrease close to 0 after 2nd lag (since AR(2))

## Week 6 video on my notes

### The stationarity of AR(1) process

+ MA process is always stationary
+ This is not true for AR process
+ When dealing with AR process, use *recursive* method to do something with phi
+ if m approaches infinity, abs(phi) < 1
+ all kinds of maths going on
+ AR(1) process is stationary

### Optimal forecast of AR(1)

+ h=1 <- horizon = 1
+ difference between realization and forecast
+ density forecast - make assumption that epsilon is normally distributed

    + optimal density forecast is also a normal distribution

+ all kinds of maths stuffs going on here
+ h=s <- what will be the forecasts? "s steps ahead"
+ density forecast - normally distributed mu_t+s/t

    + when s goes to infinity, we get normal distribution
    + unconditional mean and variance
    + AR(1) TS is a short memory process
        + when period s goes to infinity, then the information set will not have any prediction power
        + optimal forecast converts to **unconditional mean and variance**
        
### Summary

+ Differencing <- process of computing the differences between consecutive observations

    + this process can stabilize the mean of a time series by removing changes in the level of a time series
    + most cases second order differencing is sufficient to make a series stationary
    + recommended to never go beyond seconder differencing
    
+ Auto regressive <- suggests a series current points to be dependent on previous points
+ ACF <- total correlation between different lag functions

    + takes into consideration of all the past observations irrespective of its effect on the future or present time period
    + calculates the correlation between the t and (t-k) time period. 
    + includes all the lags or intervals between t and (t-k) time periods. 
    + Correlation is always calculated using the Pearson Correlation formula.
    
+ PACF <- the correlation between two lags irrespective of other lags in the series
    
    + determines the partial correlation between time period t and t-k
    + does NOT take into consideration all the time lags between t and t-k
    + only the time lags having a direct impact on future time period by neglecting the insignificant time lags in between the two-time slots t and t-k.
    
+ First bar in every correlogram <- always equal to one as this is simply measuring the variable correlated with itself
+ Distinguishing between AR vs MA:

    + Observe which graph falls below the significance line first. 
    + If the PACF graph becomes insignificant before the ACF plot then the series is mostly an AR process

+ If the PACF becomes insignificant after the second lag, it is an AR(2) process
+ If the ACF plot becomes insignificant after the 2nd bar, it is mostly a MA(2) process
